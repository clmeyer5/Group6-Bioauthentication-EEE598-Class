{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmdvX_rSrZWV",
        "outputId": "b2bf4acd-a62f-4899-e737-42f633c6b372"
      },
      "outputs": [],
      "source": [
        "!pip install wfdb\n",
        "!pip install neurokit2\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from collections import namedtuple\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import wfdb\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# Define the named tuple structure\n",
        "EMGData = namedtuple('EMGData', [\n",
        "    'id',           # Unique identifier\n",
        "    'subject',      # Subject/participant number\n",
        "    'gesture',      # Gesture number\n",
        "    'trial',        # Trial number\n",
        "    'session',      # Session number\n",
        "    'image'         # FFT image (5 windows x 32 channels x 512 freq bins)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isOnzkElvows"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 1: Download and Extract Dataset from Drive\n",
        "# ============================================================================\n",
        "\n",
        "def download_grabmyo_dataset(save_dir='grabmyo_data', drive_zip_path='/content/drive/MyDrive/gesture-recognition-and-biometrics-electromyogram-grabmyo-1.1.0.zip'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Define marker file path\n",
        "    marker_file = os.path.join(save_dir, '.extraction_complete')\n",
        "\n",
        "    # Check if extraction was already completed\n",
        "    if os.path.exists(marker_file):\n",
        "        print(f\"✓ Dataset already extracted at: {save_dir}\")\n",
        "        print(\"Skipping extraction to save time.\")\n",
        "        return save_dir\n",
        "\n",
        "    # Check if Google Drive is mounted\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive is not mounted. Please mount Google Drive to access the dataset.\")\n",
        "        print(\"You can do this by clicking the folder icon on the left, then the Google Drive icon.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Using dataset from Google Drive: {drive_zip_path}\")\n",
        "\n",
        "    if not os.path.exists(drive_zip_path):\n",
        "        print(f\"Error: Zip file not found at {drive_zip_path}\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\nExtracting files...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(drive_zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(save_dir)\n",
        "\n",
        "        print(f\"Dataset extracted to: {save_dir}\")\n",
        "\n",
        "        # Create marker file to indicate successful extraction\n",
        "        with open(marker_file, 'w') as f:\n",
        "            f.write(f\"Extraction completed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "        print(\"✓ Extraction complete! Marker file created.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during extraction: {e}\")\n",
        "        return None\n",
        "\n",
        "    return save_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDQCnmx2LfI2"
      },
      "outputs": [],
      "source": [
        "def normalize_fft_images(data_list, global_min=None, global_max=None):\n",
        "    # Compute global statistics if not provided\n",
        "    if global_min is None or global_max is None:\n",
        "        print(\"Computing global statistics from all data...\")\n",
        "        all_values = []\n",
        "        for data in data_list:\n",
        "            all_values.append(data.image.flatten())\n",
        "\n",
        "        all_values = np.concatenate(all_values)\n",
        "        global_min = all_values.min()\n",
        "        global_max = all_values.max()\n",
        "\n",
        "        print(f\"Global min: {global_min:.4f}\")\n",
        "        print(f\"Global max: {global_max:.4f}\")\n",
        "\n",
        "    # Normalize all images using global statistics\n",
        "    print(\"Normalizing all images with global statistics...\")\n",
        "    normalized_data = []\n",
        "\n",
        "    for data in data_list:\n",
        "        # Normalize to [0, 1]\n",
        "        if global_max - global_min > 0:\n",
        "            normalized_image = (data.image - global_min) / (global_max - global_min)\n",
        "        else:\n",
        "            normalized_image = np.zeros_like(data.image)\n",
        "\n",
        "        # Scale to [-1, 1]\n",
        "        normalized_image = 2 * normalized_image - 1\n",
        "\n",
        "        # Create new EMGData with normalized image\n",
        "        normalized_data.append(EMGData(\n",
        "            id=data.id,\n",
        "            subject=data.subject,\n",
        "            gesture=data.gesture,\n",
        "            trial=data.trial,\n",
        "            session=data.session,\n",
        "            image=normalized_image\n",
        "        ))\n",
        "\n",
        "    print(f\"Normalized {len(normalized_data)} samples\")\n",
        "\n",
        "    return normalized_data, global_min, global_max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBiwbhigMld2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqN8qIoTL_JW"
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_cnn(data_list):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for data in data_list:\n",
        "        # The image shape is (5, 32, 512). We need to add a channel dimension for CNN.\n",
        "        # The input shape for a 2D CNN is typically (batch_size, height, width, channels).\n",
        "        # Here, each sample is a sequence of 5 images, so we can treat this as a batch of 5 images with 1 channel each,\n",
        "        # or reshape it to have 5 channels with height 32 and width 512, or flatten the windows.\n",
        "        # Let's reshape it to (32, 512, 5) to represent 5 channels.\n",
        "        reshaped_image = np.transpose(data.image, (1, 2, 0)) # Transpose to (32, 512, 5)\n",
        "        images.append(reshaped_image)\n",
        "        labels.append(data.subject) # Use subject as the label\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return images, labels\n",
        "\n",
        "class EMGDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = torch.tensor(images, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bio1zKpuH9L7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import neurokit2 as nk\n",
        "from scipy.interpolate import interp1d\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: Channel Configuration\n",
        "# ============================================================================\n",
        "\n",
        "# Define channel indices to keep (all except unused channels)\n",
        "# Unused channels: U1-U4 at column indices 16, 23, 24, 31 (0-indexed)\n",
        "UNUSED_CHANNELS = [16, 23, 24, 31]\n",
        "# ALL_CHANNELS = list(range(32))\n",
        "# ACTIVE_CHANNELS = [ch for ch in ALL_CHANNELS if ch not in UNUSED_CHANNELS]\n",
        "\n",
        "# Channel groups for reference\n",
        "# FOREARM_CHANNELS = list(range(0, 16))  # F1-F16\n",
        "# WRIST_CHANNELS = list(range(17, 23)) + list(range(25, 31))  # W1-W12 (skipping unused)\n",
        "\n",
        "# only use these channels (0-indexed): [8,9,10,11,12,13,14,15,17,18,19,20,21,22]\n",
        "ACTIVE_CHANNELS = [8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22]\n",
        "\n",
        "# Channel groups for reference\n",
        "FOREARM_CHANNELS = [ch for ch in ACTIVE_CHANNELS if ch < 16]   # F9-F16 (0-indexed)\n",
        "WRIST_CHANNELS  = [ch for ch in ACTIVE_CHANNELS if ch >= 17]     # W1-W6  (0-indexed)\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 2: Data Augmentation Functions\n",
        "# ============================================================================\n",
        "\n",
        "def augment_emg_signal(signal, fs=2048, augmentation_params=None):\n",
        "    if augmentation_params is None:\n",
        "        augmentation_params = {\n",
        "            'noise_level': 0.02,\n",
        "            'amplitude_scale': True,\n",
        "            'time_warp': True,\n",
        "            'dc_shift': True,\n",
        "            'baseline_wander': True\n",
        "        }\n",
        "\n",
        "    augmented = signal.copy()\n",
        "\n",
        "    # 1. Add random Gaussian noise\n",
        "    if augmentation_params.get('noise_level', 0) > 0:\n",
        "        noise_std = augmentation_params['noise_level'] * np.std(signal, axis=0)\n",
        "        noise = np.random.normal(0, noise_std, signal.shape)\n",
        "        augmented += noise\n",
        "\n",
        "    # 2. Random amplitude scaling (0.8 to 1.2)\n",
        "    if augmentation_params.get('amplitude_scale', False):\n",
        "        scale_factor = np.random.uniform(0.8, 1.2)\n",
        "        augmented *= scale_factor\n",
        "\n",
        "    # 3. Small DC shift per channel\n",
        "    if augmentation_params.get('dc_shift', False):\n",
        "        dc_shift = np.random.uniform(-0.05, 0.05, augmented.shape[1])\n",
        "        augmented += dc_shift\n",
        "\n",
        "    # 4. Low frequency baseline wander (simulating motion artifacts)\n",
        "    if augmentation_params.get('baseline_wander', False):\n",
        "        t = np.arange(augmented.shape[0]) / fs\n",
        "        for ch in range(augmented.shape[1]):\n",
        "            freq = np.random.uniform(0.1, 2.0)  # 0.1-2 Hz\n",
        "            amplitude = np.random.uniform(0.01, 0.05) * np.std(signal[:, ch])\n",
        "            phase = np.random.uniform(0, 2*np.pi)\n",
        "            baseline = amplitude * np.sin(2 * np.pi * freq * t + phase)\n",
        "            augmented[:, ch] += baseline\n",
        "\n",
        "    # 5. Time warping (subtle stretching/compression)\n",
        "    if augmentation_params.get('time_warp', False):\n",
        "        warp_factor = np.random.uniform(0.95, 1.05)\n",
        "        original_time = np.arange(augmented.shape[0])\n",
        "        new_time = np.linspace(0, augmented.shape[0]-1,\n",
        "                               int(augmented.shape[0] * warp_factor))\n",
        "\n",
        "        warped = np.zeros_like(augmented)\n",
        "        for ch in range(augmented.shape[1]):\n",
        "            interp_func = interp1d(original_time, augmented[:, ch],\n",
        "                                   kind='cubic', fill_value='extrapolate')\n",
        "            warped_signal = interp_func(new_time)\n",
        "\n",
        "            # Resample back to original length\n",
        "            if len(warped_signal) > augmented.shape[0]:\n",
        "                warped[:, ch] = warped_signal[:augmented.shape[0]]\n",
        "            else:\n",
        "                warped[:len(warped_signal), ch] = warped_signal\n",
        "                warped[len(warped_signal):, ch] = warped_signal[-1]\n",
        "\n",
        "        augmented = warped\n",
        "\n",
        "    return augmented\n",
        "\n",
        "\n",
        "def generate_augmented_samples(signal, fs=2048, num_augmentations=5):\n",
        "    augmented_samples = [signal]  # Include original\n",
        "\n",
        "    # Generate variations with different augmentation strengths\n",
        "    for i in range(num_augmentations):\n",
        "        # Vary augmentation parameters\n",
        "        params = {\n",
        "            'noise_level': np.random.uniform(0.01, 0.03),\n",
        "            'amplitude_scale': np.random.choice([True, False]),\n",
        "            'time_warp': np.random.choice([True, False]),\n",
        "            'dc_shift': True,\n",
        "            'baseline_wander': np.random.choice([True, False])\n",
        "        }\n",
        "\n",
        "        augmented = augment_emg_signal(signal, fs, params)\n",
        "        augmented_samples.append(augmented)\n",
        "\n",
        "    return augmented_samples\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 3: EMG Signal Filtering\n",
        "# ============================================================================\n",
        "\n",
        "def filter_emg_signal(signal, fs=2048, method='neurokit'):\n",
        "    filtered = np.zeros_like(signal)\n",
        "\n",
        "    for ch in range(signal.shape[1]):\n",
        "        if method == 'neurokit':\n",
        "            filtered[:, ch] = nk.emg_clean(signal[:, ch], sampling_rate=fs)\n",
        "        else:\n",
        "            # Custom bandpass filter\n",
        "            filtered[:, ch] = nk.signal_filter(\n",
        "                signal[:, ch],\n",
        "                sampling_rate=fs,\n",
        "                lowcut=100,\n",
        "                highcut=500,\n",
        "                method='butterworth',\n",
        "                order=4\n",
        "            )\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 4: Updated Data Loading Functions\n",
        "# ============================================================================\n",
        "\n",
        "def parse_filename(filename):\n",
        "    parts = filename.split('_')\n",
        "    session = int(parts[0].replace('session', ''))\n",
        "    subject = int(parts[1].replace('participant', ''))\n",
        "    gesture = int(parts[2].replace('gesture', ''))\n",
        "    trial = int(parts[3].replace('trial', ''))\n",
        "    return session, subject, gesture, trial\n",
        "\n",
        "\n",
        "def create_fft_image(signals, fs, window_duration=1, target_width=512):\n",
        "    samples_per_window = int(fs * window_duration)\n",
        "    num_windows = signals.shape[0] // samples_per_window\n",
        "    num_channels = signals.shape[1]\n",
        "\n",
        "    fft_images = []\n",
        "\n",
        "    for window_idx in range(num_windows):\n",
        "        start_idx = window_idx * samples_per_window\n",
        "        end_idx = start_idx + samples_per_window\n",
        "\n",
        "        fft_image = np.zeros((num_channels, target_width))\n",
        "\n",
        "        for ch in range(num_channels):\n",
        "            window_data = signals[start_idx:end_idx, ch]\n",
        "\n",
        "            # Compute FFT\n",
        "            N = len(window_data)\n",
        "            fft_result = fft(window_data)\n",
        "            freqs = fftfreq(N, 1/fs)\n",
        "\n",
        "            # Keep only positive frequencies\n",
        "            positive_mask = freqs >= 0\n",
        "            positive_freqs = freqs[positive_mask]\n",
        "            fft_magnitude = np.abs(fft_result[positive_mask])\n",
        "\n",
        "            # Interpolate to target width\n",
        "            interp_func = interp1d(positive_freqs, fft_magnitude,\n",
        "                                   kind='linear', fill_value='extrapolate')\n",
        "            new_freqs = np.linspace(0, positive_freqs[-1], target_width)\n",
        "            fft_magnitude_resampled = interp_func(new_freqs)\n",
        "\n",
        "            fft_image[ch, :] = fft_magnitude_resampled\n",
        "\n",
        "        fft_images.append(fft_image)\n",
        "\n",
        "    fft_images = np.array(fft_images)\n",
        "\n",
        "    # Apply log transform ONLY (no normalization)\n",
        "    epsilon = 1e-10\n",
        "    fft_images_log = np.log10(fft_images + epsilon)\n",
        "\n",
        "    return fft_images_log\n",
        "\n",
        "\n",
        "def load_single_file(file_path, unique_id, apply_filtering=True,\n",
        "                     apply_augmentation=False, num_augmentations=5):\n",
        "    # Read WFDB record\n",
        "    record = wfdb.rdrecord(file_path)\n",
        "\n",
        "    # Extract metadata from filename\n",
        "    filename = os.path.basename(file_path)\n",
        "    session, subject, gesture, trial = parse_filename(filename)\n",
        "\n",
        "    # Filter channels (remove unused channels)\n",
        "    signals = record.p_signal[:, ACTIVE_CHANNELS]\n",
        "\n",
        "    # Apply filtering if requested\n",
        "    if apply_filtering:\n",
        "        signals = filter_emg_signal(signals, fs=record.fs)\n",
        "\n",
        "    data_samples = []\n",
        "\n",
        "    # Just process original signal\n",
        "    fft_image = create_fft_image(signals, record.fs)\n",
        "\n",
        "    data_samples.append(EMGData(\n",
        "        id=unique_id,\n",
        "        subject=subject,\n",
        "        gesture=gesture,\n",
        "        trial=trial,\n",
        "        session=session,\n",
        "        image=fft_image\n",
        "    ))\n",
        "\n",
        "    # Generate augmented versions if requested\n",
        "    if apply_augmentation:\n",
        "        augmented_signals = generate_augmented_samples(\n",
        "            signals, fs=record.fs, num_augmentations=num_augmentations\n",
        "        )\n",
        "\n",
        "        for aug_idx, aug_signal in enumerate(augmented_signals):\n",
        "            fft_image = create_fft_image(aug_signal, record.fs)\n",
        "\n",
        "            data_samples.append(EMGData(\n",
        "                id=unique_id,\n",
        "                subject=subject,\n",
        "                gesture=gesture,\n",
        "                trial=8 + aug_idx,  # Augmented trials start at 8\n",
        "                session=session,\n",
        "                image=fft_image\n",
        "            ))\n",
        "\n",
        "    return data_samples\n",
        "\n",
        "\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import os\n",
        "\n",
        "def load_all_data(base_dir='grabmyo_data', apply_filtering=True,\n",
        "                  apply_augmentation=False, num_augmentations=5, gesture=1,\n",
        "                  load_all_gestures=False, use_multiprocessing=True, max_workers=None):\n",
        "\n",
        "    if load_all_gestures:\n",
        "        print(f\"Scanning for ALL gesture files...\")\n",
        "    else:\n",
        "        print(f\"Scanning for gesture {gesture} files...\")\n",
        "\n",
        "    # First pass: count total files matching criteria\n",
        "    all_files = []\n",
        "    gesture_counts = {}\n",
        "\n",
        "    for root, dirs, files in os.walk(base_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.hea'):\n",
        "                filename = file[:-4]  # Remove .hea extension\n",
        "                try:\n",
        "                    _, _, file_gesture, _ = parse_filename(filename)\n",
        "\n",
        "                    # Track gesture counts\n",
        "                    gesture_counts[file_gesture] = gesture_counts.get(file_gesture, 0) + 1\n",
        "\n",
        "                    # Filter based on load_all_gestures flag\n",
        "                    if load_all_gestures or file_gesture == gesture:\n",
        "                        file_path = os.path.join(root, filename)\n",
        "                        all_files.append(file_path)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "    total_files = len(all_files)\n",
        "\n",
        "    if load_all_gestures:\n",
        "        print(f\"Found {total_files} files across {len(gesture_counts)} gestures:\")\n",
        "        for gest, count in sorted(gesture_counts.items()):\n",
        "            print(f\"  Gesture {gest}: {count} files\")\n",
        "    else:\n",
        "        print(f\"Found {total_files} files for gesture {gesture}\")\n",
        "\n",
        "    if total_files == 0:\n",
        "        print(\"No files found!\")\n",
        "        return []\n",
        "\n",
        "    # Check available CPU cores\n",
        "    import multiprocessing as mp\n",
        "    available_cores = mp.cpu_count()\n",
        "    if max_workers is None:\n",
        "        max_workers = available_cores\n",
        "\n",
        "    print(f\"\\nAvailable CPU cores: {available_cores}\")\n",
        "\n",
        "    if use_multiprocessing and total_files > 1:\n",
        "        print(f\"Using multiprocessing with {max_workers} workers\")\n",
        "    else:\n",
        "        print(\"Using single-threaded processing\")\n",
        "\n",
        "    if apply_augmentation:\n",
        "        print(f\"Augmentation enabled: {num_augmentations} additional samples per file\")\n",
        "\n",
        "    print(\"\\nLoading files...\")\n",
        "\n",
        "    data_list = []\n",
        "    files_processed = 0\n",
        "\n",
        "    if use_multiprocessing and total_files > 1:\n",
        "        # Multiprocessing approach\n",
        "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "            # Submit all tasks\n",
        "            future_to_file = {\n",
        "                executor.submit(\n",
        "                    load_single_file,\n",
        "                    file_path,\n",
        "                    idx + 1,\n",
        "                    apply_filtering,\n",
        "                    apply_augmentation,\n",
        "                    num_augmentations\n",
        "                ): file_path\n",
        "                for idx, file_path in enumerate(all_files)\n",
        "            }\n",
        "\n",
        "            # Process results as they complete\n",
        "            for future in as_completed(future_to_file):\n",
        "                file_path = future_to_file[future]\n",
        "                try:\n",
        "                    samples = future.result()\n",
        "                    data_list.extend(samples)\n",
        "                    files_processed += 1\n",
        "\n",
        "                    # Print progress every 20 files\n",
        "                    if files_processed % 20 == 0:\n",
        "                        percentage = (files_processed / total_files) * 100\n",
        "                        total_samples = len(data_list)\n",
        "                        print(f\"Progress: {files_processed}/{total_files} files ({percentage:.1f}%) - {total_samples} samples generated\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    filename = os.path.basename(file_path)\n",
        "                    print(f\"Error loading {filename}: {e}\")\n",
        "    else:\n",
        "        # Single-threaded approach (fallback)\n",
        "        for idx, file_path in enumerate(all_files):\n",
        "            try:\n",
        "                samples = load_single_file(\n",
        "                    file_path,\n",
        "                    idx + 1,\n",
        "                    apply_filtering,\n",
        "                    apply_augmentation,\n",
        "                    num_augmentations\n",
        "                )\n",
        "                data_list.extend(samples)\n",
        "                files_processed += 1\n",
        "\n",
        "                # Print progress every 20 files\n",
        "                if files_processed % 20 == 0:\n",
        "                    percentage = (files_processed / total_files) * 100\n",
        "                    total_samples = len(data_list)\n",
        "                    print(f\"Progress: {files_processed}/{total_files} files ({percentage:.1f}%) - {total_samples} samples generated\")\n",
        "\n",
        "            except Exception as e:\n",
        "                filename = os.path.basename(file_path)\n",
        "                print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "    total_count = len(data_list)\n",
        "\n",
        "    print(f\"\\n=== Loading Complete ===\")\n",
        "    if load_all_gestures:\n",
        "        print(f\"All gestures loaded\")\n",
        "        # Count samples per gesture\n",
        "        gesture_sample_counts = {}\n",
        "        for data in data_list:\n",
        "            gesture_sample_counts[data.gesture] = gesture_sample_counts.get(data.gesture, 0) + 1\n",
        "        print(\"Samples per gesture:\")\n",
        "        for gest, count in sorted(gesture_sample_counts.items()):\n",
        "            print(f\"  Gesture {gest}: {count} samples\")\n",
        "    else:\n",
        "        print(f\"Gesture: {gesture}\")\n",
        "\n",
        "    print(f\"Files processed: {files_processed}/{total_files} (100%)\")\n",
        "\n",
        "    if apply_augmentation:\n",
        "        original_samples = files_processed\n",
        "        augmented_samples = total_count - original_samples\n",
        "        print(f\"Original samples: {original_samples}\")\n",
        "        print(f\"Augmented samples: {augmented_samples}\")\n",
        "        print(f\"Total samples: {total_count}\")\n",
        "    else:\n",
        "        print(f\"Total samples: {total_count}\")\n",
        "\n",
        "    print(f\"Active channels: {len(ACTIVE_CHANNELS)} (excluding {len(UNUSED_CHANNELS)} unused)\")\n",
        "\n",
        "    return data_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86V7mCur1IUp",
        "outputId": "ccc4c7d8-eea7-4c82-b24d-eb1315825044"
      },
      "outputs": [],
      "source": [
        "# Download and extract the dataset\n",
        "dataset_path = download_grabmyo_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYGZ7KNrug31",
        "outputId": "641e5e0b-c777-4bfc-dddf-00f6d6c9de9e"
      },
      "outputs": [],
      "source": [
        "print(\"Loading data...\")\n",
        "data_vector_raw = load_all_data('grabmyo_data', apply_augmentation=True, gesture=17)\n",
        "print(f\"Loaded {len(data_vector_raw)} samples\")\n",
        "\n",
        "# Step 2: Apply global normalization to ALL data\n",
        "data_vector, global_min, global_max = normalize_fft_images(data_vector_raw)\n",
        "\n",
        "\n",
        "# Step 4: Split into train/test with new strategy\n",
        "data_vector.sort(key=lambda x: (x.subject, x.session, x.trial))\n",
        "\n",
        "# Get all unique subjects\n",
        "subjects = sorted(list(set([data.subject for data in data_vector])))\n",
        "\n",
        "# Define subject groups\n",
        "main_train_subjects = subjects[:35]  # First 35 subjects\n",
        "reserved_train_subjects = subjects[35:38]  # Next 3 subjects (36, 37, 38)\n",
        "test_only_subjects = subjects[38:]  # Remaining subjects (39+)\n",
        "\n",
        "print(f\"Main training subjects: {len(main_train_subjects)} subjects\")\n",
        "print(f\"Reserved training subjects: {len(reserved_train_subjects)} subjects\")\n",
        "print(f\"Test-only subjects: {len(test_only_subjects)} subjects\")\n",
        "\n",
        "# Initialize data lists\n",
        "train_data_main = []  # First 35 subjects, all data\n",
        "train_data_reserved = []  # Subjects 36-38, all sessions except last\n",
        "test_data = []  # Subjects 36-38 last session + all data from subjects 39+\n",
        "\n",
        "# Process first 35 subjects - all data goes to main training\n",
        "for data in data_vector:\n",
        "    if data.subject in main_train_subjects:\n",
        "        train_data_main.append(data)\n",
        "\n",
        "# Process subjects 36-38 - split by last session\n",
        "for subject in reserved_train_subjects:\n",
        "    subject_data = [d for d in data_vector if d.subject == subject]\n",
        "\n",
        "    # Get all unique sessions for this subject\n",
        "    sessions = sorted(list(set([d.session for d in subject_data])))\n",
        "\n",
        "    if len(sessions) > 0:\n",
        "        last_session = sessions[-1]\n",
        "\n",
        "        # All sessions except last → reserved training\n",
        "        for data in subject_data:\n",
        "            if data.session != last_session:\n",
        "                train_data_reserved.append(data)\n",
        "            else:\n",
        "                # Last session → test\n",
        "                test_data.append(data)\n",
        "\n",
        "# Process remaining subjects (39+) - all data goes to test\n",
        "for data in data_vector:\n",
        "    if data.subject in test_only_subjects:\n",
        "        test_data.append(data)\n",
        "\n",
        "# Print statistics\n",
        "print(f\"\\n=== Data Split Summary ===\")\n",
        "print(f\"Main training samples (subjects 1-35): {len(train_data_main)}\")\n",
        "print(f\"Reserved training samples (subjects 36-38, excluding last session): {len(train_data_reserved)}\")\n",
        "print(f\"Test samples (subjects 36-38 last session + subjects 39+): {len(test_data)}\")\n",
        "print(f\"\\nTotal training samples: {len(train_data_main) + len(train_data_reserved)}\")\n",
        "print(f\"Total test samples: {len(test_data)}\")\n",
        "\n",
        "# Optional: Verify subject distribution\n",
        "print(f\"\\n=== Subject Distribution ===\")\n",
        "train_main_subjects_actual = sorted(list(set([d.subject for d in train_data_main])))\n",
        "train_reserved_subjects_actual = sorted(list(set([d.subject for d in train_data_reserved])))\n",
        "test_subjects_actual = sorted(list(set([d.subject for d in test_data])))\n",
        "\n",
        "print(f\"Main train subjects: {train_main_subjects_actual}\")\n",
        "print(f\"Reserved train subjects: {train_reserved_subjects_actual}\")\n",
        "print(f\"Test subjects: {test_subjects_actual}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSBFX6HUPo-a",
        "outputId": "f19e3235-6f5e-42f7-befd-6c97cc7b43f2"
      },
      "outputs": [],
      "source": [
        "class ImprovedSubjectCNN(nn.Module):\n",
        "    def __init__(self, num_classes, input_shape, dropout_rate=0.5):\n",
        "        super(ImprovedSubjectCNN, self).__init__()\n",
        "\n",
        "        # Input: (batch, 5, 14, 512) after permute\n",
        "        self.conv1 = nn.Conv2d(5, 64, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # Only pool width, not height (use (1, 2) instead of (2, 2))\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 2))  # -> (64, 14, 256)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))  # -> (128, 7, 128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=(1, 2))  # -> (256, 7, 64)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=(1, 2))  # -> (512, 7, 32)\n",
        "\n",
        "        # Calculate flattened size with actual input shape\n",
        "        self.flatten = nn.Flatten()\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, *input_shape)\n",
        "            dummy = dummy.permute(0, 3, 1, 2)  # -> (1, 5, 14, 512)\n",
        "            x = self.pool1(self.relu1(self.bn1(self.conv1(dummy))))\n",
        "            x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
        "            x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
        "            x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
        "            flattened_size = self.flatten(x).shape[1]\n",
        "            print(f\"Calculated flattened size: {flattened_size}\")\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn6 = nn.BatchNorm1d(256)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2)  # (batch, height, width, channels) -> (batch, channels, height, width)\n",
        "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
        "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout1(self.relu5(self.bn5(self.fc1(x))))\n",
        "        x = self.dropout2(self.relu6(self.bn6(self.fc2(x))))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare data\n",
        "train_images, train_labels = prepare_data_for_cnn(train_data_main)\n",
        "\n",
        "# Get unique labels FIRST\n",
        "unique_labels = sorted(list(set(train_labels)))  # sorted for consistency\n",
        "\n",
        "# Create mapping from unique labels only\n",
        "subject_to_int = {subject: i for i, subject in enumerate(unique_labels)}\n",
        "\n",
        "print(f\"Number of classes: {len(unique_labels)}\")\n",
        "print(f\"Unique subjects: {unique_labels}\")\n",
        "\n",
        "# Convert labels to integers\n",
        "train_labels_int = np.array([subject_to_int[label] for label in train_labels])\n",
        "\n",
        "print(f\"\\nTraining samples: {len(train_labels_int)}\")\n",
        "print(f\"Train label range: {train_labels_int.min()} to {train_labels_int.max()}\")\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = EMGDataset(train_images, train_labels_int)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJE67F5OF-KR",
        "outputId": "6b09271f-2f05-48df-af62-7c2c634e88de"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_shape = train_images.shape[1:]\n",
        "num_classes = len(unique_labels)\n",
        "\n",
        "model = ImprovedSubjectCNN(num_classes, input_shape, dropout_rate=0.3)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "# Use StepLR scheduler - reduces LR every 15 epochs\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
        "\n",
        "# Or alternatively, use CosineAnnealingLR for smooth decay\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "print(f\"Model initialized on {device}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Input shape: {input_shape}\")\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "best_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        running_corrects += torch.sum(predicted == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Get current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}, LR: {current_lr:.6f}')\n",
        "\n",
        "    # Save best model based on training loss\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'  Saved best model with loss: {best_loss:.4f}\\n')\n",
        "\n",
        "print(f\"\\nFinished Training. Best training loss: {best_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OItYyC9hz6Cf"
      },
      "outputs": [],
      "source": [
        "class BiometricVerificationModel(nn.Module):\n",
        "    def __init__(self, pretrained_model_path=\"best_model.pth\", dropout_rate=0.3, device=\"cpu\"):\n",
        "        super(BiometricVerificationModel, self).__init__()\n",
        "\n",
        "        checkpoint = torch.load(pretrained_model_path, map_location=device)\n",
        "\n",
        "        # Extract original parameters\n",
        "        original_num_classes = checkpoint['fc3.weight'].shape[0]\n",
        "        original_flattened_size = checkpoint['fc1.weight'].shape[1]\n",
        "\n",
        "        print(f\"Loading pretrained model:\")\n",
        "        print(f\"  - Num classes: {original_num_classes}\")\n",
        "        print(f\"  - Flattened size: {original_flattened_size}\")\n",
        "\n",
        "        if original_flattened_size == 114688:\n",
        "            original_input_shape = (14, 512, 5)\n",
        "        else:\n",
        "            # Fallback\n",
        "            original_input_shape = (14, 512, 5)\n",
        "            print(f\"  - WARNING: Unexpected flattened size, using shape: {original_input_shape}\")\n",
        "\n",
        "        # Create and load original model\n",
        "        original_model = ImprovedSubjectCNN(\n",
        "            num_classes=original_num_classes,\n",
        "            input_shape=original_input_shape,\n",
        "            dropout_rate=0.5\n",
        "        )\n",
        "        original_model.load_state_dict(checkpoint)\n",
        "\n",
        "        # Copy all layers\n",
        "        self.conv1 = original_model.conv1\n",
        "        self.bn1 = original_model.bn1\n",
        "        self.relu1 = original_model.relu1\n",
        "        self.pool1 = original_model.pool1\n",
        "\n",
        "        self.conv2 = original_model.conv2\n",
        "        self.bn2 = original_model.bn2\n",
        "        self.relu2 = original_model.relu2\n",
        "        self.pool2 = original_model.pool2\n",
        "\n",
        "        self.conv3 = original_model.conv3\n",
        "        self.bn3 = original_model.bn3\n",
        "        self.relu3 = original_model.relu3\n",
        "        self.pool3 = original_model.pool3\n",
        "\n",
        "        self.conv4 = original_model.conv4\n",
        "        self.bn4 = original_model.bn4\n",
        "        self.relu4 = original_model.relu4\n",
        "        self.pool4 = original_model.pool4\n",
        "\n",
        "        self.flatten = original_model.flatten\n",
        "        self.fc1 = original_model.fc1\n",
        "        self.bn5 = original_model.bn5\n",
        "        self.relu5 = original_model.relu5\n",
        "        self.dropout1 = original_model.dropout1\n",
        "\n",
        "        self.fc2 = original_model.fc2\n",
        "        self.bn6 = original_model.bn6\n",
        "        self.relu6 = original_model.relu6\n",
        "        self.dropout2 = original_model.dropout2\n",
        "\n",
        "        # Freeze pretrained layers\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # Add new verification head\n",
        "        self.fc_verify1 = nn.Linear(256, 128)\n",
        "        self.bn_verify1 = nn.BatchNorm1d(128)\n",
        "        self.relu_verify1 = nn.ReLU()\n",
        "        self.dropout_verify1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.fc_verify2 = nn.Linear(128, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Make verification head trainable\n",
        "        for p in [self.fc_verify1, self.bn_verify1, self.fc_verify2]:\n",
        "            for param in p.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
        "        x = self.pool4(self.relu4(self.bn4(self.conv4(x))))\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout1(self.relu5(self.bn5(self.fc1(x))))\n",
        "        x = self.dropout2(self.relu6(self.bn6(self.fc2(x))))\n",
        "\n",
        "        # Verification head\n",
        "        x = self.dropout_verify1(self.relu_verify1(self.bn_verify1(self.fc_verify1(x))))\n",
        "        x = self.fc_verify2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq99-WaQz8_P",
        "outputId": "c30edcb1-f4fc-4109-877e-35e1ca58e602"
      },
      "outputs": [],
      "source": [
        "# Choose one of the reserved training subjects as the target verification subject\n",
        "# reserved_train_subjects was defined when splitting data_vector\n",
        "TARGET_SUBJECT = reserved_train_subjects[2]  # or manually: 36 / 37 / 38\n",
        "\n",
        "# Get all data for the target subject from the reserved+test sets\n",
        "target_all = [d for d in train_data_reserved if d.subject == TARGET_SUBJECT] + \\\n",
        "             [d for d in test_data if d.subject == TARGET_SUBJECT]\n",
        "\n",
        "# Find this subject's sessions\n",
        "target_sessions = sorted(list(set(d.session for d in target_all)))\n",
        "assert len(target_sessions) >= 2, \"Target subject must have at least 2 sessions.\"\n",
        "\n",
        "# Define training and test sessions:\n",
        "# - training: first two sessions\n",
        "# - test: last session (may be same as third if only 3 sessions)\n",
        "train_sessions_target = target_sessions[:2]\n",
        "test_session_target = target_sessions[-1]\n",
        "\n",
        "genuine_train = [d for d in target_all if d.session in train_sessions_target]\n",
        "genuine_test = [d for d in target_all if d.session == test_session_target]\n",
        "\n",
        "print(f\"Target subject: {TARGET_SUBJECT}\")\n",
        "print(f\"Target sessions: {target_sessions}\")\n",
        "print(f\"Train sessions (target): {train_sessions_target}\")\n",
        "print(f\"Test session (target): {test_session_target}\")\n",
        "print(f\"Genuine train samples: {len(genuine_train)}\")\n",
        "print(f\"Genuine test samples: {len(genuine_test)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Impostor pools\n",
        "# -----------------------------\n",
        "\n",
        "# 1) Training impostors: subjects from main training set ONLY (subjects 1-35),\n",
        "#    and we can optionally restrict to the same sessions as target's train sessions\n",
        "impostor_train_pool = [\n",
        "    d for d in train_data_main\n",
        "    if d.subject != TARGET_SUBJECT and d.session in train_sessions_target\n",
        "]\n",
        "\n",
        "# 2) Test impostors:\n",
        "#    - other 2 reserved subjects: only their last session\n",
        "#    - all test_only_subjects (39+) all sessions\n",
        "#    - optionally: last session from main train subjects as \"unseen\" sessions, if present in test_data\n",
        "other_reserved_subjects = [s for s in reserved_train_subjects if s != TARGET_SUBJECT]\n",
        "\n",
        "impostor_test_pool = []\n",
        "\n",
        "# Other reserved subjects: get their last session and put into impostor pool\n",
        "for subj in other_reserved_subjects:\n",
        "    subj_all = [d for d in train_data_reserved if d.subject == subj] + \\\n",
        "               [d for d in test_data if d.subject == subj]\n",
        "    if len(subj_all) == 0:\n",
        "        continue\n",
        "    subj_sessions = sorted(list(set(d.session for d in subj_all)))\n",
        "    last_sess = subj_sessions[-1]\n",
        "    impostor_test_pool.extend([d for d in subj_all if d.session == last_sess])\n",
        "\n",
        "# Test-only subjects (39+): everything is impostor test data\n",
        "test_only_subjects = [s for s in sorted(list(set(d.subject for d in test_data)))\n",
        "                      if s not in reserved_train_subjects]\n",
        "impostor_test_pool.extend([d for d in test_data if d.subject in test_only_subjects])\n",
        "\n",
        "print(f\"Impostor train pool: {len(impostor_train_pool)}\")\n",
        "print(f\"Impostor test pool: {len(impostor_test_pool)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91PtwY830Dpr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_training_pairs(ratio_impostor_to_genuine=1.0):\n",
        "    pairs = []\n",
        "\n",
        "    # Genuine pairs (target subject vs itself)\n",
        "    for gs in genuine_train:\n",
        "        pairs.append((gs, 1.0))  # label=1.0 for genuine\n",
        "\n",
        "    n_impostor_needed = int(ratio_impostor_to_genuine * len(genuine_train))\n",
        "    for _ in range(n_impostor_needed):\n",
        "        isample = random.choice(impostor_train_pool)\n",
        "        pairs.append((isample, 0.0))  # label=0.0 for impostor\n",
        "\n",
        "    random.shuffle(pairs)\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def generate_test_pairs(ratio_impostor_to_genuine=4.0):\n",
        "    pairs = []\n",
        "\n",
        "    for gs in genuine_test:\n",
        "        pairs.append((gs, 1.0))\n",
        "\n",
        "    n_impostor_needed = int(ratio_impostor_to_genuine * len(genuine_test))\n",
        "    for _ in range(n_impostor_needed):\n",
        "        isample = random.choice(impostor_test_pool)\n",
        "        pairs.append((isample, 0.0))\n",
        "\n",
        "    random.shuffle(pairs)\n",
        "    return pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRGF2tM70GrG"
      },
      "outputs": [],
      "source": [
        "class VerificationDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample, label = self.pairs[idx]\n",
        "        # sample.image: (5, 32, 512) -> (32, 512, 5)\n",
        "        img = np.transpose(sample.image, (1, 2, 0)).astype(np.float32)\n",
        "        img = torch.from_numpy(img)\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "        return img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqDmSbrw0JoZ",
        "outputId": "8f8e1979-3bec-4a7d-81f2-6d69325a6013"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = BiometricVerificationModel(pretrained_model_path=\"best_model.pth\", device=device).to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # Regenerate training pairs each epoch for fresh impostors\n",
        "    train_pairs = generate_training_pairs(ratio_impostor_to_genuine=1.0)\n",
        "    train_dataset = VerificationDataset(train_pairs)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device).view(-1, 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - train loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# Build test loader once (or regenerate if you want random impostors)\n",
        "test_pairs = generate_test_pairs(ratio_impostor_to_genuine=4.0)\n",
        "test_dataset = VerificationDataset(test_pairs)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device).view(-1, 1)\n",
        "\n",
        "        y_hat = model(x)\n",
        "        all_preds.append(y_hat.cpu())\n",
        "        all_labels.append(y.cpu())\n",
        "\n",
        "all_preds = torch.cat(all_preds).numpy().ravel()\n",
        "all_labels = torch.cat(all_labels).numpy().ravel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00oxSe2J6-6S",
        "outputId": "e9d389a4-6bc7-4a26-a9f6-d8c1c98aad1a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def comprehensive_validation(verification_model, device, target_subject,\n",
        "                           train_data_main, train_data_reserved, test_data):\n",
        "\n",
        "    verification_model.eval()\n",
        "\n",
        "    # Get target subject's data\n",
        "    target_all = [d for d in train_data_reserved if d.subject == target_subject] + \\\n",
        "                 [d for d in test_data if d.subject == target_subject]\n",
        "\n",
        "    # Find target subject's sessions\n",
        "    target_sessions = sorted(list(set(d.session for d in target_all)))\n",
        "    train_sessions_target = target_sessions[:2]  # Sessions 1 and 2\n",
        "    test_session_target = target_sessions[-1]    # Last session (session 3)\n",
        "\n",
        "    # Get test-only subjects (the \"untrained upper ones\")\n",
        "    test_only_subjects = sorted(list(set(d.subject for d in test_data if d.subject >= 39)))\n",
        "\n",
        "    print(f\"Target subject: {target_subject}\")\n",
        "    print(f\"Target sessions: {target_sessions}\")\n",
        "    print(f\"Train sessions: {train_sessions_target}\")\n",
        "    print(f\"Test session: {test_session_target}\")\n",
        "    print(f\"Test-only subjects (39+): {test_only_subjects}\")\n",
        "\n",
        "    # Define thresholds (50% to 100% in 5% increments)\n",
        "    thresholds = np.arange(0.00, 1.05, 0.05)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # ========================================================================\n",
        "    # SCENARIO A: All data (Used to see overtrain and dataaugmentation effects)\n",
        "    # ========================================================================\n",
        "    print(\"\\n=== SCENARIO A: All data ===\")\n",
        "\n",
        "    all_data = train_data_main + train_data_reserved + test_data\n",
        "    pairs_a = generate_all_data_pairs(all_data, target_subject)\n",
        "    predictions_a, labels_a = get_predictions(verification_model, pairs_a, device)\n",
        "    results['A'] = create_confusion_matrices(predictions_a, labels_a, thresholds, \"A: All data\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SCENARIO B: All data except sessions 1&2 of target subject\n",
        "    # ========================================================================\n",
        "    print(\"\\n=== SCENARIO B: All data except sessions 1&2 of target subject ===\")\n",
        "\n",
        "    # Remove sessions 1&2 of target subject\n",
        "    filtered_data_b = []\n",
        "    for d in all_data:\n",
        "        if d.subject == target_subject and d.session in train_sessions_target:\n",
        "            continue  # Skip sessions 1&2 of target subject\n",
        "        filtered_data_b.append(d)\n",
        "\n",
        "    pairs_b = generate_all_data_pairs(filtered_data_b, target_subject)\n",
        "    predictions_b, labels_b = get_predictions(verification_model, pairs_b, device)\n",
        "    results['B'] = create_confusion_matrices(predictions_b, labels_b, thresholds,\n",
        "                                           \"B: All data except target sessions 1&2\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SCENARIO C: Non-augmented data only, excluding sessions 1&2 of target\n",
        "    # ========================================================================\n",
        "    print(\"\\n=== SCENARIO C: Non-augmented data only, excluding target sessions 1&2 ===\")\n",
        "\n",
        "    # Filter to non-augmented data only (no \"_aug\" in ID)\n",
        "    non_aug_data = [d for d in filtered_data_b if \"_aug\" not in str(d.id)]\n",
        "\n",
        "    pairs_c = generate_all_data_pairs(non_aug_data, target_subject)\n",
        "    predictions_c, labels_c = get_predictions(verification_model, pairs_c, device)\n",
        "    results['C'] = create_confusion_matrices(predictions_c, labels_c, thresholds,\n",
        "                                           \"C: Non-augmented only, excluding target sessions 1&2\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SCENARIO D: Only session 3 of target + test-only subjects (39+)\n",
        "    # ========================================================================\n",
        "    print(\"\\n=== SCENARIO D: Target session 3 + test-only subjects (39+) ===\")\n",
        "\n",
        "    # Target subject session 3\n",
        "    target_session3 = [d for d in target_all if d.session == test_session_target]\n",
        "\n",
        "    # Test-only subjects (39+)\n",
        "    test_only_data = [d for d in test_data if d.subject in test_only_subjects]\n",
        "\n",
        "    scenario_d_data = target_session3 + test_only_data\n",
        "\n",
        "    pairs_d = generate_all_data_pairs(scenario_d_data, target_subject)\n",
        "    predictions_d, labels_d = get_predictions(verification_model, pairs_d, device)\n",
        "    results['D'] = create_confusion_matrices(predictions_d, labels_d, thresholds,\n",
        "                                           \"D: Target session 3 + test-only subjects\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # SCENARIO E: Non-augmented data only - Target session 3 + test-only subjects (Results from this)\n",
        "    # ========================================================================\n",
        "    print(\"\\n=== SCENARIO E: Non-augmented only - Target session 3 + test-only subjects ===\")\n",
        "\n",
        "    # Get target subject's data\n",
        "    target_all = [d for d in train_data_reserved if d.subject == target_subject] + \\\n",
        "                 [d for d in test_data if d.subject == target_subject]\n",
        "\n",
        "    target_sessions = sorted(list(set(d.session for d in target_all)))\n",
        "    test_session_target = target_sessions[-1]    # Last session (session 3)\n",
        "\n",
        "    # Get test-only subjects (39+)\n",
        "    test_only_subjects = sorted(list(set(d.subject for d in test_data if d.subject >= 39)))\n",
        "\n",
        "    # Target subject session 3 - NON-AUGMENTED ONLY\n",
        "    target_session3_orig = [d for d in target_all\n",
        "                           if d.session == test_session_target and d.trial <= 7]\n",
        "\n",
        "    # Test-only subjects - NON-AUGMENTED ONLY\n",
        "    test_only_data_orig = [d for d in test_data\n",
        "                          if d.subject in test_only_subjects and d.trial <= 7]\n",
        "\n",
        "    scenario_e_data = target_session3_orig + test_only_data_orig\n",
        "\n",
        "    print(f\"Target session 3 (original only): {len(target_session3_orig)} samples\")\n",
        "    print(f\"Test-only subjects (original only): {len(test_only_data_orig)} samples\")\n",
        "    print(f\"Total scenario E samples: {len(scenario_e_data)}\")\n",
        "\n",
        "    pairs_e = generate_all_data_pairs(scenario_e_data, target_subject)\n",
        "    predictions_e, labels_e = get_predictions(verification_model, pairs_e, device)\n",
        "    results['E'] = create_confusion_matrices(predictions_e, labels_e, thresholds,\n",
        "                                           \"E: Non-augmented only - Target session 3 + test-only subjects\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def generate_all_data_pairs(data_pool, target_subject):\n",
        "    pairs = []\n",
        "\n",
        "    for sample in data_pool:\n",
        "        if sample.subject == target_subject:\n",
        "            pairs.append((sample, 1.0))  # Genuine\n",
        "        else:\n",
        "            pairs.append((sample, 0.0))  # Impostor\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def get_predictions(model, pairs, device, batch_size=64):\n",
        "    dataset = VerificationDataset(pairs)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x = x.to(device)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            all_predictions.append(y_pred.cpu().numpy())\n",
        "            all_labels.append(y.numpy())\n",
        "\n",
        "    predictions = np.concatenate(all_predictions).ravel()\n",
        "    labels = np.concatenate(all_labels).ravel()\n",
        "\n",
        "    return predictions, labels\n",
        "\n",
        "\n",
        "def create_confusion_matrices(predictions, labels, thresholds, scenario_name):\n",
        "    print(f\"\\n{scenario_name}\")\n",
        "    print(f\"Total samples: {len(predictions)}\")\n",
        "    print(f\"Genuine samples: {np.sum(labels == 1)}\")\n",
        "    print(f\"Impostor samples: {np.sum(labels == 0)}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for threshold in thresholds:\n",
        "        # Convert predictions to binary decisions\n",
        "        predicted_labels = (predictions >= threshold).astype(int)\n",
        "\n",
        "        # Create confusion matrix\n",
        "        cm = confusion_matrix(labels, predicted_labels, labels=[0, 1])\n",
        "\n",
        "        plot_confusion_matrix(cm, threshold, scenario_name)\n",
        "\n",
        "        # Extract values\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        # False Accept Rate (FAR) and False Reject Rate (FRR)\n",
        "        far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "        frr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "        results.append({\n",
        "            'Threshold': f\"{threshold:.2f}\",\n",
        "            'TP': tp,\n",
        "            'TN': tn,\n",
        "            'FP': fp,\n",
        "            'FN': fn,\n",
        "            'Accuracy': f\"{accuracy:.3f}\",\n",
        "            'Precision': f\"{precision:.3f}\",\n",
        "            'Recall': f\"{recall:.3f}\",\n",
        "            'Specificity': f\"{specificity:.3f}\",\n",
        "            'F1': f\"{f1:.3f}\",\n",
        "            'FAR': f\"{far:.3f}\",\n",
        "            'FRR': f\"{frr:.3f}\"\n",
        "        })\n",
        "\n",
        "    # Create DataFrame and print table\n",
        "    df = pd.DataFrame(results)\n",
        "    print(f\"\\nConfusion Matrix Results for {scenario_name}:\")\n",
        "    print(\"=\" * 120)\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "def run_comprehensive_validation():\n",
        "    results = comprehensive_validation(\n",
        "        verification_model=model,\n",
        "        device=device,\n",
        "        target_subject=TARGET_SUBJECT,\n",
        "        train_data_main=train_data_main,\n",
        "        train_data_reserved=train_data_reserved,\n",
        "        test_data=test_data\n",
        "    )\n",
        "\n",
        "    # Save results to files if needed\n",
        "    for scenario, df in results.items():\n",
        "        df.to_csv(f\"validation_results_scenario_{scenario}.csv\", index=False)\n",
        "        print(f\"\\nSaved results for scenario {scenario} to validation_results_scenario_{scenario}.csv\")\n",
        "\n",
        "    return results\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix(cm: np.ndarray, threshold: float, scenario_name: str,\n",
        "                         save_dir: str = \"cm_plots\"):\n",
        "    import os, pathlib\n",
        "    pathlib.Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    labels = ['Impostor (0)', 'Genuine (1)']\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=labels, yticklabels=labels, cbar=False)\n",
        "    plt.title(f'{scenario_name}\\nThreshold = {threshold:.2f}')\n",
        "    plt.ylabel('True'); plt.xlabel('Predicted')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    safe_name = scenario_name.replace(' ', '_').replace(':', '')\n",
        "    fpath = os.path.join(save_dir, f'{safe_name}_th{threshold:.2f}.png')\n",
        "    plt.savefig(fpath, dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "# Run the validation\n",
        "validation_results = run_comprehensive_validation()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNgRcekKYr0D",
        "outputId": "3e05592f-9a1a-4373-9309-84474bd249b4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Filter data for subject 36, trial 1, gesture 1, session 1\n",
        "target_samples = [d for d in data_vector\n",
        "                  if d.subject == 35\n",
        "                  and d.trial == 1\n",
        "                  and d.gesture == 1\n",
        "                  and d.session == 1]\n",
        "\n",
        "print(f\"Found {len(target_samples)} matching samples\")\n",
        "\n",
        "if len(target_samples) > 0:\n",
        "    # Take the first matching sample\n",
        "    sample = target_samples[0]\n",
        "\n",
        "    print(f\"\\nSample details:\")\n",
        "    print(f\"  ID: {sample.id}\")\n",
        "    print(f\"  Subject: {sample.subject}\")\n",
        "    print(f\"  Gesture: {sample.gesture}\")\n",
        "    print(f\"  Trial: {sample.trial}\")\n",
        "    print(f\"  Session: {sample.session}\")\n",
        "    print(f\"  Image shape: {sample.image.shape}\")  # (5 windows, 14 channels, 512 freq bins)\n",
        "\n",
        "    # Plot all 5 windows\n",
        "    fig, axes = plt.subplots(5, 1, figsize=(15, 20))\n",
        "    fig.suptitle(f'Subject {sample.subject}, Gesture {sample.gesture}, Trial {sample.trial}, Session {sample.session}',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    for window_idx in range(5):\n",
        "        ax = axes[window_idx]\n",
        "\n",
        "        # Get the FFT image for this window (14 channels x 512 freq bins)\n",
        "        window_data = sample.image[window_idx, :, :]\n",
        "\n",
        "        # Plot as heatmap\n",
        "        im = ax.imshow(window_data, aspect='auto', cmap='viridis',\n",
        "                      extent=[0, 512, len(ACTIVE_CHANNELS), 0])\n",
        "\n",
        "        ax.set_title(f'Window {window_idx + 1}')\n",
        "        ax.set_xlabel('Frequency Bin')\n",
        "        ax.set_ylabel('Channel Index')\n",
        "        ax.set_yticks(np.arange(len(ACTIVE_CHANNELS)) + 0.5)\n",
        "        ax.set_yticklabels([f'Ch {ch}' for ch in ACTIVE_CHANNELS])\n",
        "\n",
        "        # Add colorbar\n",
        "        plt.colorbar(im, ax=ax, label='Log Magnitude (Normalized)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('subject36_trial1_gesture1_session1.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Also create a summary plot showing all windows side-by-side\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(25, 6))\n",
        "    fig.suptitle(f'All Windows - Subject {sample.subject}, Gesture {sample.gesture}, Trial {sample.trial}, Session {sample.session}',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "\n",
        "    for window_idx in range(5):\n",
        "        ax = axes[window_idx]\n",
        "        window_data = sample.image[window_idx, :, :]\n",
        "\n",
        "        im = ax.imshow(window_data, aspect='auto', cmap='viridis',\n",
        "                      extent=[0, 512, len(ACTIVE_CHANNELS), 0])\n",
        "        ax.set_title(f'Window {window_idx + 1}')\n",
        "        ax.set_xlabel('Freq Bin')\n",
        "        ax.set_ylabel('Channel')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('subject36_trial1_gesture1_session1_summary.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No matching samples found!\")\n",
        "    print(\"\\nAvailable data for subject 36:\")\n",
        "    subj36_data = [d for d in data_vector if d.subject == 36]\n",
        "    if subj36_data:\n",
        "        for d in subj36_data[:10]:  # Show first 10\n",
        "            print(f\"  Subject {d.subject}, Gesture {d.gesture}, Trial {d.trial}, Session {d.session}\")\n",
        "    else:\n",
        "        print(\"  No data found for subject 36\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlPFCaJUxjpA",
        "outputId": "b9db007e-88df-4dd5-d86a-b042bd1f2d62"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------\n",
        "# Grad-CAM – FULLY DETERMINISTIC (seed everything)\n",
        "# ----------------------------------------------------------\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ---- FORCE DETERMINISM ----\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ---- 1. Load model ONCE ----\n",
        "model = BiometricVerificationModel(\n",
        "    pretrained_model_path='best_model.pth',\n",
        "    dropout_rate=0.3,\n",
        "    device=device\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# Explicitly set all dropout/batchnorm to eval\n",
        "for m in model.modules():\n",
        "    if isinstance(m, (torch.nn.Dropout, torch.nn.BatchNorm2d, torch.nn.BatchNorm1d)):\n",
        "        m.eval()\n",
        "        for p in m.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "# Unfreeze only conv4 + bn4\n",
        "for p in model.conv4.parameters():\n",
        "    p.requires_grad = True\n",
        "for p in model.bn4.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# ---- 2. GradCAM Class ----\n",
        "class GradCAM:\n",
        "    def __init__(self, model, layer):\n",
        "        self.model = model\n",
        "        self.layer = layer\n",
        "        self.acts = None\n",
        "        self.grads = None\n",
        "        self._f = layer.register_forward_hook(self._save_act)\n",
        "        self._b = layer.register_full_backward_hook(self._save_grad)\n",
        "\n",
        "    def _save_act(self, m, inp, out):\n",
        "        self.acts = out.detach().clone()\n",
        "\n",
        "    def _save_grad(self, m, gin, gout):\n",
        "        self.grads = gout[0].detach().clone()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.model.zero_grad()\n",
        "        self.acts = None\n",
        "        self.grads = None\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            out = self.model(x)\n",
        "\n",
        "        out.backward()\n",
        "\n",
        "        g = self.grads[0]\n",
        "        a = self.acts[0]\n",
        "        w = g.mean(dim=(1, 2))\n",
        "        cam = torch.relu(torch.einsum('c,chw->hw', w, a))\n",
        "\n",
        "        return cam.cpu().numpy()\n",
        "\n",
        "    def cleanup(self):\n",
        "        self._f.remove()\n",
        "        self._b.remove()\n",
        "\n",
        "# ---- 3. Helper function ----\n",
        "def pick_sample(s, sess, t, g):\n",
        "    h = [d for d in data_vector\n",
        "         if d.subject == s and d.session == sess and d.trial == t and d.gesture == g]\n",
        "    return h[0] if h else None\n",
        "\n",
        "# ---- 4. Main visualization loop ----\n",
        "for subj in (35, 36, 37, 38):\n",
        "    samp = pick_sample(subj, 1, 1, 1)\n",
        "    if samp is None:\n",
        "        print(f'⚠ No data for subject {subj}')\n",
        "        continue\n",
        "\n",
        "    # samp.image shape: (5, 14, 512) = (C, H, W)\n",
        "    print(f\"Subject {subj} - Original shape: {samp.image.shape}\")\n",
        "\n",
        "    # Generate Grad-CAM ONCE for the full image\n",
        "    # Match your original format: (H, W, C)\n",
        "    full_img_transposed = samp.image.transpose(1, 2, 0)  # (14, 512, 5)\n",
        "    full_img_tensor = torch.from_numpy(full_img_transposed.copy()).unsqueeze(0).float().to(device)\n",
        "    full_img_tensor.requires_grad_(True)\n",
        "\n",
        "    print(f\"Full image tensor shape: {full_img_tensor.shape}\")\n",
        "\n",
        "    # Generate heatmap for full image\n",
        "    gradcam = GradCAM(model, model.conv4)\n",
        "    heatmap_full = gradcam(full_img_tensor)\n",
        "    gradcam.cleanup()\n",
        "\n",
        "    # Resize heatmap to match full image dimensions (14, 512)\n",
        "    heatmap_full_resized = cv2.resize(heatmap_full, (512, 14))\n",
        "\n",
        "    # Now split into windows\n",
        "    num_windows = 5\n",
        "    window_width = 512 // num_windows  # 102 pixels per window\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "    fig.suptitle(f'Subject {subj}', fontsize=14)\n",
        "\n",
        "    for w, ax in enumerate(axes):\n",
        "        # Extract window region from original image\n",
        "        start_col = w * window_width\n",
        "        end_col = start_col + window_width\n",
        "\n",
        "        # Extract spectrogram window (C, H, W)\n",
        "        spec_window = samp.image[:, :, start_col:end_col]  # (5, 14, 102)\n",
        "\n",
        "        # Extract corresponding heatmap region\n",
        "        heatmap_window = heatmap_full_resized[:, start_col:end_col]  # (14, 102)\n",
        "\n",
        "        # Visualize\n",
        "        spec_avg = spec_window.mean(axis=0)  # Average across channels: (14, 102)\n",
        "        ax.imshow(spec_avg, cmap='gray', aspect='auto')\n",
        "        ax.imshow(heatmap_window, cmap='Reds', aspect='auto', alpha=0.45)\n",
        "        ax.set_title(f'Second {w + 1}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f'✓ Subject {subj} completed\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "TTDIdvChVh0Q",
        "outputId": "86cba37c-ee2e-42e7-bf3f-5944d1a1cc77"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1.  Re-create the raw counts from the description\n",
        "# -------------------------------------------------\n",
        "TP = 3\n",
        "TN = 105\n",
        "FP = 0\n",
        "FN = 4\n",
        "\n",
        "cm = np.array([[TN, FP],\n",
        "               [FN, TP]])\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2.  Row-normalise → per-class accuracy\n",
        "# -------------------------------------------------\n",
        "cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, None] + 1e-9)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3.  Build percentage + count labels\n",
        "# -------------------------------------------------\n",
        "labels = [f\"{v:.1%}\\n({c})\" for v, c in zip(cm_norm.ravel(), cm.ravel())]\n",
        "labels = np.asarray(labels).reshape(2, 2)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4.  Plot\n",
        "# -------------------------------------------------\n",
        "plt.figure(figsize=(5.5, 4))\n",
        "sns.heatmap(cm_norm,\n",
        "            annot=labels,\n",
        "            fmt='',\n",
        "            cmap='Blues',\n",
        "            vmin=0,\n",
        "            vmax=1,\n",
        "            xticklabels=['Predicted Impostor', 'Predicted Genuine'],\n",
        "            yticklabels=['Actual Impostor', 'Actual Genuine'])\n",
        "plt.title('Weighted Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5.  Quick summary\n",
        "# -------------------------------------------------\n",
        "print(f\"Impostor Accuracy : {TN/(TN+FP):.1%}\")\n",
        "print(f\"Genuine Accuracy  : {TP/(TP+FN):.1%}\")\n",
        "print(f\"FAR               : {FP/(TN+FP):.4f}\")\n",
        "print(f\"FRR               : {FN/(TP+FN):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8QGamCJkkMW",
        "outputId": "cb7bde91-0dee-4923-9835-de88d4933248"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------\n",
        "# Grad-CAM – Different heatmap regions for each 1-second segment (Take 2)\n",
        "# ----------------------------------------------------------\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ---- FORCE DETERMINISM ----\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ---- 1. Load model ONCE ----\n",
        "model = BiometricVerificationModel(\n",
        "    pretrained_model_path='best_model.pth',\n",
        "    dropout_rate=0.3,\n",
        "    device=device\n",
        ").to(device)\n",
        "model.eval()\n",
        "\n",
        "# Explicitly set all dropout/batchnorm to eval\n",
        "for m in model.modules():\n",
        "    if isinstance(m, (torch.nn.Dropout, torch.nn.BatchNorm2d, torch.nn.BatchNorm1d)):\n",
        "        m.eval()\n",
        "        for p in m.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "# Unfreeze only conv4 + bn4\n",
        "for p in model.conv4.parameters():\n",
        "    p.requires_grad = True\n",
        "for p in model.bn4.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# ---- 2. GradCAM Class ----\n",
        "class GradCAM:\n",
        "    def __init__(self, model, layer):\n",
        "        self.model = model\n",
        "        self.layer = layer\n",
        "        self.acts = None\n",
        "        self.grads = None\n",
        "        self._f = layer.register_forward_hook(self._save_act)\n",
        "        self._b = layer.register_full_backward_hook(self._save_grad)\n",
        "\n",
        "    def _save_act(self, m, inp, out):\n",
        "        self.acts = out.detach().clone()\n",
        "\n",
        "    def _save_grad(self, m, gin, gout):\n",
        "        self.grads = gout[0].detach().clone()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.model.zero_grad()\n",
        "        self.acts = None\n",
        "        self.grads = None\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            out = self.model(x)\n",
        "\n",
        "        out.backward()\n",
        "\n",
        "        g = self.grads[0]\n",
        "        a = self.acts[0]\n",
        "        w = g.mean(dim=(1, 2))\n",
        "        cam = torch.relu(torch.einsum('c,chw->hw', w, a))\n",
        "\n",
        "        return cam.cpu().numpy()\n",
        "\n",
        "    def cleanup(self):\n",
        "        self._f.remove()\n",
        "        self._b.remove()\n",
        "\n",
        "# ---- 3. Helper function ----\n",
        "def pick_sample(s, sess, t, g):\n",
        "    h = [d for d in data_vector\n",
        "         if d.subject == s and d.session == sess and d.trial == t and d.gesture == g]\n",
        "    return h[0] if h else None\n",
        "\n",
        "# ---- 4. Main visualization loop ----\n",
        "for subj in (35, 36, 37, 38):\n",
        "    samp = pick_sample(subj, 1, 1, 1)\n",
        "    if samp is None:\n",
        "        print(f'⚠ No data for subject {subj}')\n",
        "        continue\n",
        "\n",
        "    # samp.image shape: (5, 14, 512)\n",
        "    print(f\"Subject {subj} - samp.image shape: {samp.image.shape}\")\n",
        "\n",
        "    # Generate Grad-CAM ONCE for the full image\n",
        "    full_img_transposed = samp.image.transpose(1, 2, 0)  # (14, 512, 5)\n",
        "    full_img_tensor = torch.from_numpy(full_img_transposed.copy()).unsqueeze(0).float().to(device)\n",
        "    full_img_tensor.requires_grad_(True)\n",
        "\n",
        "    # Generate heatmap for full image\n",
        "    gradcam = GradCAM(model, model.conv4)\n",
        "    heatmap_full = gradcam(full_img_tensor)\n",
        "    gradcam.cleanup()\n",
        "\n",
        "    # Resize heatmap to match full image dimensions\n",
        "    heatmap_full_resized = cv2.resize(heatmap_full, (512, 14))\n",
        "\n",
        "    # Divide heatmap horizontally into 5 sections (one per second)\n",
        "    num_windows = 5\n",
        "    window_width = 512 // num_windows  # 102 pixels per window\n",
        "\n",
        "    # Create figure: 1 row × 5 columns (one per second)\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "    fig.suptitle(f'Subject {subj}', fontsize=14)\n",
        "\n",
        "    for sec in range(5):\n",
        "        ax = axes[sec]\n",
        "\n",
        "        # Extract channel (which IS the 1-second segment)\n",
        "        spec_second = samp.image[sec, :, :]  # (14, 512)\n",
        "\n",
        "        # Extract corresponding section of the heatmap\n",
        "        start_col = sec * window_width\n",
        "        end_col = start_col + window_width\n",
        "        heatmap_window = heatmap_full_resized[:, start_col:end_col]  # (14, 102)\n",
        "\n",
        "        # Visualize this second's spectrogram with its heatmap overlay\n",
        "        ax.imshow(spec_second, cmap='gray', aspect='auto')\n",
        "        ax.imshow(heatmap_window, cmap='Reds', aspect='auto', alpha=0.45)\n",
        "        ax.set_title(f'Second {sec + 1}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f'✓ Subject {subj} completed\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "f8RRlDPFZTtW",
        "outputId": "875030e4-be44-4fbe-b5f8-526c1fa527b9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['XGBoost', 'Random Forest', 'Autoencoder\\n+ MLP', 'CNN']\n",
        "f1 = [55.16, 55.01, 59.7, 82.1]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "bars = plt.bar(models, f1, color=['#4c72b0', '#55a868', '#c44e52', '#8172b2'])\n",
        "\n",
        "# label each bar with its value\n",
        "for b, v in zip(bars, f1):\n",
        "    plt.text(b.get_x() + b.get_width()/2, b.get_height() + 1,\n",
        "             f'{v:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.ylabel('F1 score (%)')\n",
        "plt.title('Model comparison')\n",
        "plt.ylim(0, 100)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# plt.savefig('f1_bars.png', dpi=300)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
